# Task 6: Web Scraping and Job Data Analysis

from bs4 import BeautifulSoup
import pandas as pd
from collections import Counter

# Step 1: Load the sample HTML job listings file
with open("job_sample.html", "r", encoding="utf-8") as f:
    html = f.read()

# Step 2: Parse the HTML using BeautifulSoup
soup = BeautifulSoup(html, "html.parser")

# Step 3: Extract all job listings
jobs = soup.find_all("div", class_="job-listing")

# Step 4: Extract job title, company, location, and skills from each job
job_data = []
for job in jobs:
    title = job.find("h2", class_="title").text.strip()
    company = job.find("span", class_="company").text.strip()
    location = job.find("span", class_="location").text.strip()
    skills = job.find("div", class_="skills").text.strip()
    job_data.append([title, company, location, skills])

# Step 5: Convert to a pandas DataFrame
df = pd.DataFrame(job_data, columns=["Job Title", "Company", "Location", "Skills"])
print("Extracted Data:")
print(df)

# Step 6: Analyze top skills from all listings
all_skills = []
for skills in df["Skills"]:
    all_skills.extend([s.strip() for s in skills.split(",")])

top_skills = Counter(all_skills).most_common(5)
print("\nTop 5 In-Demand Skills:")
for skill, count in top_skills:
    print(f"{skill}: {count}")

# Step 7 (Optional): Save output to CSV
df.to_csv("scraped_jobs.csv", index=False)  # CSV file with extracted job data
